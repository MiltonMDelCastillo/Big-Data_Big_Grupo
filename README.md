# Proyecto Big Data - Sensores IoT

Proyecto para procesar y almacenar datos de sensores IoT usando Kafka y MongoDB. Optimizado para manejar mÃ¡s de 1 millÃ³n de registros.

## ğŸ“‹ Requisitos Previos

- Python 3.8 o superior
- Docker y Docker Compose
- ConexiÃ³n a Internet (para MongoDB Atlas)

## ğŸš€ Pasos para Abrir el Proyecto

### 1. Instalar Dependencias

Abre una terminal en la raÃ­z del proyecto y ejecuta:

```bash
pip install -r requirements.txt
```

O instala las dependencias individualmente:

```bash
pip install kafka-python==2.0.2
pip install pandas==2.0.3
pip install flask==2.3.3
pip install flask-cors==4.0.0
pip install pymongo==4.6.0
pip install numpy==1.24.3
pip install dnspython==2.4.2
```

### 2. Iniciar Kafka y MongoDB (Docker)

Navega a la carpeta `Infra` y ejecuta Docker Compose:

```bash
cd Infra
docker-compose up -d
```

Esto iniciarÃ¡:
- **Zookeeper** (puerto 2181)
- **Kafka** (puerto 9092)
- **Kafka UI** (puerto 8080) - Interfaz web para ver los datos
- **MongoDB Local** (puerto 27017) - Opcional, se usa MongoDB Atlas

Verifica que los contenedores estÃ©n corriendo:

```bash
docker-compose ps
```

### 3. Verificar ConexiÃ³n a MongoDB Atlas

El proyecto estÃ¡ configurado para usar MongoDB Atlas. La conexiÃ³n ya estÃ¡ configurada en el cÃ³digo:

```
mongodb+srv://fabricabla_db_user:ifMIBidJuyoCai24@cluster0.e0tjitb.mongodb.net/
```

Base de datos: `sensores_iot`

### 4. Procesar los Archivos CSV

Navega a la carpeta `csv-producers` y ejecuta el script de procesamiento:

```bash
cd csv-producers
python producer_sensores.py
```

> ğŸ’¡ **Sin Kafka?** Si solo quieres poblar MongoDB (por ejemplo en pruebas sin Docker),
> ejecuta el script con la variable `ENABLE_KAFKA=false`:
>
> ```bash
> set ENABLE_KAFKA=false   # En PowerShell: $env:ENABLE_KAFKA="false"
> python producer_sensores.py
> ```

> ğŸ¯ **Cargar solo ciertos sensores?** Usa `SENSORES_ACTIVOS` con la lista deseada.
> Por ejemplo, para cargar Ãºnicamente aire y sonido:
> ```bash
> set SENSORES_ACTIVOS=sonido,calidad-aire
> python producer_sensores.py
> ```

> ğŸ“ **Cambiar el lÃ­mite por CSV?** Define `MAX_REGISTROS_POR_ARCHIVO`.  
> Ejemplo para procesar todo el archivo (sin lÃ­mite):
> ```bash
> set MAX_REGISTROS_POR_ARCHIVO=0
> python producer_sensores.py
> ```

Este script:
- Lee los archivos CSV de la carpeta `data/`
- Limpia y procesa los datos
- EnvÃ­a los datos a Kafka (topics: `topic-soterrados`, `topic-sonido`, `topic-calidad-aire`)
- Guarda los datos en MongoDB Atlas usando **bulk insert** (optimizado para grandes volÃºmenes)

**Archivos procesados:** (solo se cargan los que existan en `data/`)
- `data/subterraneo/EM310-UDL-915M soterrados nov 2024.csv` â†’ Sensores soterrados
- `data/sonido/WS302-915M SONIDO NOV 2024.csv` â†’ Sensores de sonido
- `data/aire/EM500-CO2-915M nov 2024.csv` â†’ Sensores de calidad de aire

> â±ï¸ **LÃ­mite por archivo:** por defecto se procesan mÃ¡x. 200â€¯000 registros de cada CSV para evitar saturar los servicios. Cambia el valor con `MAX_REGISTROS_POR_ARCHIVO`.

### 5. (Opcional) Ejecutar la API

En otra terminal, ejecuta la API para consultar los datos:

```bash
cd csv-producers
python api_sensores.py
```

La API estarÃ¡ disponible en: `http://localhost:5000`

**Endpoints disponibles:**
- `GET /` - InformaciÃ³n de la API
- `GET /api/sensores/tipos` - Listar tipos de sensores
- `GET /api/sensores/<tipo>` - Datos de un sensor especÃ­fico
- `GET /api/sensores/todos` - Todos los datos
- `GET /api/sensores/estadisticas` - EstadÃ­sticas generales

## âš¡ Optimizaciones Implementadas

El script ha sido optimizado para manejar mÃ¡s de 1 millÃ³n de registros:

1. **Bulk Insert en MongoDB**: Usa `insert_many()` en lugar de `insert_one()`, insertando 1000 documentos por lote
2. **Chunks mÃ¡s grandes**: Lee 5000 registros del CSV a la vez (antes 1000)
3. **CompresiÃ³n en Kafka**: Mensajes comprimidos con gzip (se puede desactivar con `ENABLE_KAFKA=false`)
4. **Pool de conexiones**: MongoDB con pool de 10-50 conexiones
5. **Ãndices automÃ¡ticos**: Se crean Ã­ndices en MongoDB para mejorar consultas
6. **Manejo de errores**: ContinÃºa procesando aunque haya errores en algunos registros

## ğŸ“Š Monitoreo

- **Kafka UI**: http://localhost:8080 - Ver topics y mensajes en tiempo real
- **MongoDB Atlas**: Accede a tu cluster en MongoDB Atlas para ver los datos almacenados

## ğŸ—‚ï¸ Estructura del Proyecto

```
Big-Data_Big_Grupo/
â”œâ”€â”€ csv-producers/
â”‚   â”œâ”€â”€ producer_sensores.py    # Script principal de procesamiento
â”‚   â”œâ”€â”€ limpieza_datos.py        # MÃ³dulo de limpieza de datos
â”‚   â””â”€â”€ api_sensores.py          # API REST para consultar datos
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ subterraneo/             # Sensores soterrados
â”‚   â”œâ”€â”€ sonido/                  # Sensores de sonido
â”‚   â””â”€â”€ aire/                    # Sensores de calidad de aire
â”œâ”€â”€ Infra/
â”‚   â””â”€â”€ docker-compose.yml      # ConfiguraciÃ³n Docker (Kafka, MongoDB)
â”œâ”€â”€ requirements.txt            # Dependencias Python
â””â”€â”€ README.md                   # Este archivo
```

## ğŸ”§ ConfiguraciÃ³n de Rendimiento

Puedes ajustar los parÃ¡metros de rendimiento en `producer_sensores.py`:

```python
CHUNK_SIZE_CSV = 5000        # TamaÃ±o de chunk para leer CSV
BATCH_SIZE_MONGODB = 1000    # TamaÃ±o de lote para MongoDB
BATCH_SIZE_KAFKA = 5000      # TamaÃ±o de lote para Kafka
MAX_REGISTROS_POR_ARCHIVO = 200000  # LÃ­mite por CSV (usa 0 para desactivar)
```

## âš ï¸ SoluciÃ³n de Problemas

### Error: "Kafka no disponible"
- Verifica que Docker estÃ© corriendo: `docker ps`
- Inicia los servicios: `cd Infra && docker-compose up -d`

### Error: "MongoDB Atlas no disponible"
- Verifica tu conexiÃ³n a Internet
- Verifica las credenciales en el cÃ³digo
- AsegÃºrate de que tu IP estÃ© en la whitelist de MongoDB Atlas

### El proceso es muy lento
- Aumenta `BATCH_SIZE_MONGODB` (ej: 2000 o 5000)
- Aumenta `CHUNK_SIZE_CSV` (ej: 10000)
- Verifica tu conexiÃ³n a Internet (MongoDB Atlas)

## ğŸ“ Notas

- Los datos se almacenan en MongoDB Atlas (cloud), no en el MongoDB local de Docker
- El MongoDB local en Docker es opcional y no se usa por defecto
- Los datos se envÃ­an tanto a Kafka como a MongoDB
- El script procesa los archivos secuencialmente

## ğŸ‘¥ Autores

Proyecto Big Data - Grupo Big


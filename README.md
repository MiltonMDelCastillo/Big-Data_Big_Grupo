# Proyecto Big Data - Sensores IoT

Proyecto para procesar y almacenar datos de sensores IoT usando Kafka y MongoDB. Optimizado para manejar m√°s de 1 mill√≥n de registros.

## üìã Requisitos Previos

- Python 3.8 o superior
- Docker y Docker Compose
- Conexi√≥n a Internet (para MongoDB Atlas)

## üöÄ Pasos para Abrir el Proyecto

### 1. Instalar Dependencias

Abre una terminal en la ra√≠z del proyecto y ejecuta:

```bash
pip install -r requirements.txt
```

O instala las dependencias individualmente:

```bash
pip install kafka-python==2.0.2
pip install pandas==2.0.3
pip install flask==2.3.3
pip install flask-cors==4.0.0
pip install pymongo==4.6.0
pip install numpy==1.24.3
pip install dnspython==2.4.2
```

### 2. Iniciar Kafka y MongoDB (Docker)

Navega a la carpeta `Infra` y ejecuta Docker Compose:

```bash
cd Infra
docker-compose up -d
```

Esto iniciar√°:
- **Zookeeper** (puerto 2181)
- **Kafka** (puerto 9092)
- **Kafka UI** (puerto 8080) - Interfaz web para ver los datos
- **MongoDB Local** (puerto 27017) - Opcional, se usa MongoDB Atlas

Verifica que los contenedores est√©n corriendo:

```bash
docker-compose ps
```

### 3. Verificar Conexi√≥n a MongoDB Atlas

El proyecto est√° configurado para usar MongoDB Atlas. La conexi√≥n ya est√° configurada en el c√≥digo:

```
mongodb+srv://fabricabla_db_user:ifMIBidJuyoCai24@cluster0.e0tjitb.mongodb.net/
```

Base de datos: `sensores_iot`

### 4. Procesar los Archivos CSV

Navega a la carpeta `csv-producers` y ejecuta el script de procesamiento:

```bash
cd csv-producers
python producer_sensores.py
```

Este script:
- Lee los archivos CSV de la carpeta `data/`
- Limpia y procesa los datos
- Env√≠a los datos a Kafka (topics: `topic-soterrados`, `topic-sonido`, `topic-calidad-aire`)
- Guarda los datos en MongoDB Atlas usando **bulk insert** (optimizado para grandes vol√∫menes)

**Archivos procesados:**
- `data/subterraneo/EM310-UDL-915M soterrados nov 2024.csv` ‚Üí Sensores soterrados
- `data/sonido/WS302-915M SONIDO NOV 2024.csv` ‚Üí Sensores de sonido
- `data/aire/EM500-CO2-915M nov 2024.csv` ‚Üí Sensores de calidad de aire

### 5. (Opcional) Ejecutar la API

En otra terminal, ejecuta la API para consultar los datos:

```bash
cd csv-producers
python api_sensores.py
```

La API estar√° disponible en: `http://localhost:5000`

**Endpoints disponibles:**
- `GET /` - Informaci√≥n de la API
- `GET /api/sensores/tipos` - Listar tipos de sensores
- `GET /api/sensores/<tipo>` - Datos de un sensor espec√≠fico
- `GET /api/sensores/todos` - Todos los datos
- `GET /api/sensores/estadisticas` - Estad√≠sticas generales

## ‚ö° Optimizaciones Implementadas

El script ha sido optimizado para manejar m√°s de 1 mill√≥n de registros:

1. **Bulk Insert en MongoDB**: Usa `insert_many()` en lugar de `insert_one()`, insertando 1000 documentos por lote
2. **Chunks m√°s grandes**: Lee 5000 registros del CSV a la vez (antes 1000)
3. **Compresi√≥n en Kafka**: Mensajes comprimidos con gzip
4. **Pool de conexiones**: MongoDB con pool de 10-50 conexiones
5. **√çndices autom√°ticos**: Se crean √≠ndices en MongoDB para mejorar consultas
6. **Manejo de errores**: Contin√∫a procesando aunque haya errores en algunos registros

## üìä Monitoreo

- **Kafka UI**: http://localhost:8080 - Ver topics y mensajes en tiempo real
- **MongoDB Atlas**: Accede a tu cluster en MongoDB Atlas para ver los datos almacenados

## üóÇÔ∏è Estructura del Proyecto

```
Big-Data_Big_Grupo/
‚îú‚îÄ‚îÄ csv-producers/
‚îÇ   ‚îú‚îÄ‚îÄ producer_sensores.py    # Script principal de procesamiento
‚îÇ   ‚îú‚îÄ‚îÄ limpieza_datos.py        # M√≥dulo de limpieza de datos
‚îÇ   ‚îî‚îÄ‚îÄ api_sensores.py          # API REST para consultar datos
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ subterraneo/             # Sensores soterrados
‚îÇ   ‚îú‚îÄ‚îÄ sonido/                  # Sensores de sonido
‚îÇ   ‚îî‚îÄ‚îÄ aire/                    # Sensores de calidad de aire
‚îú‚îÄ‚îÄ Infra/
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml      # Configuraci√≥n Docker (Kafka, MongoDB)
‚îú‚îÄ‚îÄ requirements.txt            # Dependencias Python
‚îî‚îÄ‚îÄ README.md                   # Este archivo
```

## üîß Configuraci√≥n de Rendimiento

Puedes ajustar los par√°metros de rendimiento en `producer_sensores.py`:

```python
CHUNK_SIZE_CSV = 5000        # Tama√±o de chunk para leer CSV
BATCH_SIZE_MONGODB = 1000    # Tama√±o de lote para MongoDB
BATCH_SIZE_KAFKA = 5000      # Tama√±o de lote para Kafka
```

## ‚ö†Ô∏è Soluci√≥n de Problemas

### Error: "Kafka no disponible"
- Verifica que Docker est√© corriendo: `docker ps`
- Inicia los servicios: `cd Infra && docker-compose up -d`

### Error: "MongoDB Atlas no disponible"
- Verifica tu conexi√≥n a Internet
- Verifica las credenciales en el c√≥digo
- Aseg√∫rate de que tu IP est√© en la whitelist de MongoDB Atlas

### El proceso es muy lento
- Aumenta `BATCH_SIZE_MONGODB` (ej: 2000 o 5000)
- Aumenta `CHUNK_SIZE_CSV` (ej: 10000)
- Verifica tu conexi√≥n a Internet (MongoDB Atlas)

## üìù Notas

- Los datos se almacenan en MongoDB Atlas (cloud), no en el MongoDB local de Docker
- El MongoDB local en Docker es opcional y no se usa por defecto
- Los datos se env√≠an tanto a Kafka como a MongoDB
- El script procesa los archivos secuencialmente

## üë• Autores

Proyecto Big Data - Grupo Big

